{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTO-DO : Vectorize the datasets cuz computer cant read inglis :(\\n        Create a ipynb file to be imported by the model creators\\n        if ipynb acts weird, create .py extension\\n    \\nWe shall use idf vectorizer cause its better than count vectorizer in almost every way\\nHash vectorization will be use as they require less memory and are faster (because they are sparse and use hashes \\nrather than tokens)\\nAlso contains \"plot_confusion_matrix\"\\n\\n\\nIMPORTANT : IMPORT THIS FILE FOR MODEL CREATION \\n            Variables of tfidf : tfidf_mega_train_x, tfidf_mega_test_x, tfidf_mega_dev_x \\n                                 tfidf_mega_train_y, tfidf_mega_test_y, tfidf_mega_dev_y \\n                                    \\n            Varaibles of Hash : hash_mega_train_x, hash_mega_test_x, hash_mega_dev_x\\n                                hash_mega_train_y, hash_mega_test_y, hash_mega_dev_y\\n\\n\\n\\nWARNING : ANY CHANGES MADE HERE OR IN \"Dataset_Preprocessing_2\" SHALL BE REQUIRED TO BE RE-IMPORTED \\n        I.E RERUN \"import import_ipynb\"\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TO-DO : Vectorize the datasets cuz computer cant read inglis :(\n",
    "        Create a ipynb file to be imported by the model creators\n",
    "        if ipynb acts weird, create .py extension\n",
    "    \n",
    "We shall use idf vectorizer cause its better than count vectorizer in almost every way\n",
    "Hash vectorization will be use as they require less memory and are faster (because they are sparse and use hashes \n",
    "rather than tokens)\n",
    "Also contains \"plot_confusion_matrix\"\n",
    "\n",
    "\n",
    "IMPORTANT : IMPORT THIS FILE FOR MODEL CREATION \n",
    "            Variables of tfidf : tfidf_mega_train_x, tfidf_mega_test_x, tfidf_mega_dev_x \n",
    "                                 tfidf_mega_train_y, tfidf_mega_test_y, tfidf_mega_dev_y \n",
    "                                    \n",
    "            Varaibles of Hash : hash_mega_train_x, hash_mega_test_x, hash_mega_dev_x\n",
    "                                hash_mega_train_y, hash_mega_test_y, hash_mega_dev_y\n",
    "\n",
    "\n",
    "\n",
    "WARNING : ANY CHANGES MADE HERE OR IN \"Dataset_Preprocessing_2\" SHALL BE REQUIRED TO BE RE-IMPORTED \n",
    "        I.E RERUN \"import import_ipynb\"\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Dataset_Preprocessing_2.ipynb\n",
      "Megaset size 70628\n",
      "Columns :  Index(['Rating', 'Statement'], dtype='object')\n",
      "Number of true statements :  18814\n",
      "Number of false statements :  16500\n",
      "(35314,)\n",
      "(35314,)\n",
      "31782\n",
      "3532\n",
      "28250\n",
      "3532\n",
      "<class 'pandas.core.series.Series'>\n",
      "Final size of train/dev/test :  28250 / 3532 / 3532\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "import import_ipynb\n",
    "import Dataset_Preprocessing_2 as dp2 #imported for lists because csv does not work with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28250\n",
      "(28250,)\n",
      "<class 'pandas.core.series.Series'>\n",
      "(28250,)\n"
     ]
    }
   ],
   "source": [
    "# mega_test_x : Final testing file with statements\n",
    "# mega_test_y : Final testing file with ratings\n",
    "# mega_train_x : Training file with statements\n",
    "# mega_train_y : Training file with ratings\n",
    "# mega_dev_x   : Development testing file with statements\n",
    "# mega_dev_y   : Development testing file with ratings\n",
    "\n",
    "mega_test_x=dp2.mega_test_x \n",
    "mega_test_y=dp2.mega_test_y\n",
    "mega_train_x=dp2.mega_train_x \n",
    "mega_train_y=dp2.mega_train_y \n",
    "mega_dev_x=dp2.mega_dev_x\n",
    "mega_dev_y=dp2.mega_dev_y\n",
    "\n",
    "print(mega_train_x.size)\n",
    "print(mega_train_x.shape)\n",
    "print(type(mega_train_x))\n",
    "\n",
    "print(mega_train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the `tfidf_vectorizer` \n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_df=0.7)   \n",
    "\n",
    "# This removes words which appear in more than 70% of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the training data \n",
    "tfidf_mega_train_x = tfidf_vectorizer.fit_transform(mega_train_x) \n",
    "tfidf_mega_test_x = tfidf_vectorizer.transform(mega_test_x) \n",
    "tfidf_mega_dev_x = tfidf_vectorizer.transform(mega_dev_x) \n",
    "#print(tfidf_mega_dev_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tfidf_mega_train_y.shape)\n",
    "#print(tfidf_mega_train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreate two models , one that uses hash and other that uses TFID for all unique algorithms\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create two models , one that uses hash and other that uses TFID for all unique algorithms\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the `Hash_vectorizer` \n",
    "\n",
    "hash_vectorizer = HashingVectorizer(stop_words='english')\n",
    "\n",
    "hash_mega_train_x = np.absolute(hash_vectorizer.fit_transform(mega_train_x))\n",
    "\n",
    "hash_mega_test_x = np.absolute(hash_vectorizer.transform(mega_test_x))\n",
    "hash_mega_dev_x = np.absolute(hash_vectorizer.transform(mega_dev_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<28250x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2177883 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_mega_train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "#this function is better as it includes normalization as well\n",
    "# ALERT : This normalization is done only on the values of the graph ,not on the data set :/\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    See full source and example: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    \n",
    "    from matplotlib import pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
